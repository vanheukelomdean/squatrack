{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cpm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAPhLtImALLz"
      },
      "source": [
        "Mount google drive and count images files in dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYGV9ZdxcSFQ",
        "outputId": "b9f5cd27-6cd5-46a9-c605-25298a8bd2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive_dir = \"/content/drive/My Drive/Datasets/\"\n",
        "targz_path = \"mpii_human_pose_v1.tar.gz\"\n",
        "extract_dir = \"images/\"\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "path = drive_dir + extract_dir\n",
        "file_count = len([f for f in os.listdir(path)if os.path.isfile(os.path.join(path, f))])\n",
        "\n",
        "print (\"There are \" + str(file_count) + \" files in directory.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "There are 5488 files in directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BLH7x648lVX"
      },
      "source": [
        "Importing utilities and libraries for file systems, datasets, image processing, and learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j51MLxOT7oW6",
        "outputId": "f3363464-9879-4eec-d008-0eaa9902a665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "!pip install console-progressbar\n",
        "\n",
        "import time\n",
        "import itertools\n",
        "from os.path import join\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from console_progressbar import ProgressBar\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Layer\n",
        "from keras.losses import MeanSquaredError\n",
        "print(\"\\nTF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting console-progressbar\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/8d/810e5c5dbdefc92cc1819d0b6ffac2c9c149acece9b3e55e4d9d05d0bb2a/console_progressbar-1.1.2.tar.gz\n",
            "Building wheels for collected packages: console-progressbar\n",
            "  Building wheel for console-progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for console-progressbar: filename=console_progressbar-1.1.2-cp36-none-any.whl size=4142 sha256=4c8727b58c1f3a424601762858c28ce452df60d664b1ac6447fdbb8a01ee06e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/48/4c/dfcbbd70b7a1690c7113e01fa2201a809203078d96de82b900\n",
            "Successfully built console-progressbar\n",
            "Installing collected packages: console-progressbar\n",
            "Successfully installed console-progressbar-1.1.2\n",
            "\n",
            "TF version: 2.3.0\n",
            "Hub version: 0.9.0\n",
            "WARNING:tensorflow:From <ipython-input-3-0e39d9ace6c7>:27: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q2oMZatV9C9"
      },
      "source": [
        "\n",
        "![Convolutional Pose Machine](https://drive.google.com/uc?export=view&id=1WrWE-qi-I5Q5GJOANEFYgUYQsqoiM_zQ)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct2T1t4HQA18"
      },
      "source": [
        "Define the convolutional pose machine architecture:\n",
        "\n",
        " - It has n stages where n > 1\n",
        "\n",
        " - Initial stage is comprised of:\n",
        "     - Interleaved convolutional and pooling layers for feature recognition with translational invariance\n",
        "     - Two fully connected layers for classification of the most abstract features\n",
        " - Each subsequent stage is comprised of \n",
        "    - Interleaved convolutional and pooling layers for feature recognition performed on the input image\n",
        "    - Concatenation of the belief maps from above and the previous stage\n",
        "    - Three further convolutional layers\n",
        "    - Two fully connected layers for classification of the most abstract features\n",
        "    - A intermediate loss calculation to cache\n",
        " - The stage loss is aggregated in place of end-of-model loss calculattion to reduce the risk of the vanishing gradient problem encountered in some feed forward neural nets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF1neBf5Fcao"
      },
      "source": [
        "class ConvPoseMachine (keras.models.Sequential):\n",
        "  def __init__(self, num_stages, num_parts, loss_function = MeanSquaredError()):\n",
        "      super(ConvPoseMachine, self).__init__()\n",
        "      assert num_stages > 1, \"There must be at least an initial and subsequent stage\"\n",
        "      self.num_stages = num_stages\n",
        "      self.num_parts = num_parts\n",
        "      self.loss_function = loss_function\n",
        "      self.stage = 0\n",
        "      self.loss = 0\n",
        "      self.total_loss = 0\n",
        "      self.build_model()\n",
        "      \n",
        "  def build_model(self):\n",
        "    self.shared_layers = []\n",
        "    self.stage_layers  = []\n",
        "\n",
        "    filters = self.num_parts\n",
        "\n",
        "    \"\"\"\n",
        "        Shared Layers\n",
        "    \"\"\"\n",
        "    # Triple 9x9 conv encoder\n",
        "    self.shared_layers.append(Conv2D(filters,\n",
        "                                      name='conv-shared-1-9x9',\n",
        "                                      kernel_size=9,\n",
        "                                      activation='relu'))\n",
        "    \n",
        "    self.shared_layers.append(MaxPooling2D(name='pool-shared-2',\n",
        "                                            pool_size=(2, 2)))\n",
        "    \n",
        "    self.shared_layers.append(Conv2D(filters,\n",
        "                                      name='conv-shared-2-9x9',\n",
        "                                      kernel_size=9,\n",
        "                                      activation='relu'))\n",
        "    \n",
        "    self.shared_layers.append(MaxPooling2D(name='pool-shared-2',\n",
        "                                            pool_size=(2, 2)))\n",
        "    \n",
        "    self.shared_layers.append(Conv2D(filters,\n",
        "                                      name='conv-shared-3-9x9',\n",
        "                                      kernel_size=9,\n",
        "                                      activation='relu'))\n",
        "    \n",
        "    self.shared_layers.append(MaxPooling2D(name='pool-shared-3',\n",
        "                                            pool_size=(2, 2)))\n",
        "\n",
        "    self.shared_layers.append(Conv2D(filters,\n",
        "                                      name=\"conv-shared-4-5x5\",\n",
        "                                      kernel_size=5,\n",
        "                                      activation='relu'))\n",
        "    \n",
        "    \"\"\"\n",
        "        Initial Stage\n",
        "    \"\"\"\n",
        "\n",
        "    self.stage_layers.append([])\n",
        "\n",
        "    \"\"\" self.stage_layers[0].append(Conv2D(filters,\n",
        "                                        name=\"conv-0-9x9\", \n",
        "                                        kernel_size=9,\n",
        "                                        activation='relu')) \"\"\"\n",
        "    \n",
        "    # Fully connnected layer for classification of the most abstract features\n",
        "    self.stage_layers[0].append(Dense(filters,\n",
        "                                name=\"dense-0-1\",\n",
        "                                activation='relu'))\n",
        "    self.stage_layers[0].append(Dense(filters,\n",
        "                                name=\"dense-0-2\",\n",
        "                                activation='relu'))\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "        Subsequent Stages\n",
        "    \"\"\"\n",
        "    for stage in range (1, self.num_stages):\n",
        "      self.stage_layers.append([])\n",
        "      for i in range (3):\n",
        "        self.stage_layers[stage].append(Conv2D(filters,\n",
        "                                                name=\"conv-\" + str(stage) + \"-11x11\",\n",
        "                                                kernel_size=11,\n",
        "                                                activation='relu'))\n",
        "        \n",
        "      # Fully connnected layer for classification of the most abstract features\n",
        "      self.stage_layers[stage].append(Dense(filters,\n",
        "                                            name=\"dense-\" + str(stage) + \"-1\",\n",
        "                                            activation='relu'))\n",
        "      self.stage_layers[stage].append(Dense(filters,\n",
        "                                            name=\"dense-\" + str(stage) +\"-2\",\n",
        "                                            activation='relu'))\n",
        "                        \n",
        "    \n",
        "\n",
        "  def call(self, image, y_true):\n",
        "    for stage in range(self.num_stages):\n",
        "      x = tf.Variable(image)\n",
        "      # Run shared layers on image\n",
        "      for layer in self.shared_layers:\n",
        "        print(layer.name + \" - input shape: \" + str(x.shape))\n",
        "        x = layer(x)\n",
        "\n",
        "      if stage == 0:\n",
        "        for layer in self.stage_layers[stage]:\n",
        "          print(layer.name + \" - input shape: \" + str(x.shape))\n",
        "          x = layer(x)\n",
        "\n",
        "        y_true_pdf = build_heatmaps(y_true.numpy(), x.shape[1])\n",
        "\n",
        "        self.stage_loss(x, y_true_pdf)\n",
        "\n",
        "      else: \n",
        "        x = tf.math.add(x, x_n_1)\n",
        "\n",
        "        for layer in self.stage_layers[stage]:\n",
        "          print(layer.name + \" - input shape: \" + str(x.shape))\n",
        "          x = layer(x)\n",
        "\n",
        "        y_true_pdf = build_heatmaps(y_true.numpy(), x.shape[1])\n",
        "\n",
        "        self.stage_loss(x, y_true)\n",
        "\n",
        "      x_n_1 = tf.Variable(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  def stage_loss (self, y_true, y_pred):\n",
        "    loss = self.loss_function(y_true, y_pred).numpy()\n",
        "\n",
        "    assert loss > 0, \"Stage loss less than zero\"\n",
        "\n",
        "    self.loss = loss\n",
        "    self.total_loss += self.loss\n",
        "\n",
        "    print(\"Stage %d loss: %f\\n\"% (self.stage, self.loss))\n",
        "\n",
        "    self.stage += 1"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afShn1AKUGVt"
      },
      "source": [
        "Utility functions for drawing gaussian belief maps, normalizing images, plotting joints on the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTGbq60IQY2d"
      },
      "source": [
        "def build_heatmaps(batch_keypoints, heatmap_dim):\n",
        "  print(heatmap_dim)\n",
        "\n",
        "  scale = 368 // heatmap_dim\n",
        "  batch_keypoints = batch_keypoints / scale\n",
        "\n",
        "  num_features = batch_keypoints.shape[1]\n",
        "  heatmaps = np.zeros((batch_keypoints.shape[0], heatmap_dim, heatmap_dim, num_features), dtype=float)\n",
        "\n",
        "  for image_keypoints in batch_keypoints: \n",
        "    for j in range (num_features):\n",
        "      heatmaps[ :,:, j] = keypoint_gaussian(heatmaps[:,:,j], image_keypoints[j])\n",
        "\n",
        "  return heatmaps\n",
        "\n",
        "\n",
        "def keypoint_gaussian(image, kp, var = 1) :\n",
        "  y_bound, x_bound = image.shape[:2]\n",
        "\n",
        "  # Calculate range of gaussian \n",
        "  top_left = [int(kp[0] - 3 * var), \n",
        "              int(kp[1] - 3 * var)]\n",
        "  bottom_right = [int(kp[0] + 3 * var + 1), \n",
        "                  int(kp[1] + 3 * var + 1)]\n",
        "  \n",
        "  # Check if entire gaussian distribution is out of image range\n",
        "  if top_left[0] > x_bound or top_left[1] > y_bound \\\n",
        "    or bottom_right[0] < 0 or bottom_right[0] < 0:\n",
        "    return image\n",
        "\n",
        "  # Compute 2 Dimensional Gaussian (un-normalized)\n",
        "  range = 6 * var + 1\n",
        "  x = np.arange(range, dtype=float)\n",
        "  x_c = x - (range//2)\n",
        "  y_c = x_c[:, np.newaxis]\n",
        "  g = np.exp(- (x_c ** 2 + y_c ** 2) / (2 * var ** 2))\n",
        "\n",
        "  # gaussian range in image\n",
        "  g_x = max(0, -top_left[0]), min(bottom_right[0], x_bound) - top_left[0]\n",
        "  g_y = max(0, -top_left[1]), min(bottom_right[1], y_bound) - top_left[1]\n",
        "\n",
        "  # Image range\n",
        "  i_x = max(0, top_left[0]), min(bottom_right[0], x_bound)\n",
        "  i_y = max(0, top_left[1]), min(bottom_right[1], y_bound)\n",
        "\n",
        "  # Copy gaussian to image\n",
        "  image [i_y[0]:i_y[1], i_x[0]:i_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "  return image\n",
        "  \n",
        "\n",
        "def normalize (image):\n",
        "  # Reduce magnitude\n",
        "  image = image / 255.0\n",
        "\n",
        "  # Center around channel mean\n",
        "  channel_mean = [0,0,0]\n",
        "  for i in range(3):\n",
        "    channel_mean[i] = image[:,:,i].mean(axis=(0, 1))\n",
        "    image[:, :, i] -= channel_mean[i]\n",
        "\n",
        "  assert np.max(image) < 1 and np.min(image) > -1, \"Problem normalizing image\"\n",
        "\n",
        "  return image.astype(np.float32)\n",
        "\n",
        "\n",
        "def visualize_heatmaps(heatmaps):\n",
        "  kron_prod_dim = (368 // heatmaps.shape[0], 368 // heatmaps.shape[1])\n",
        "  print(kron_prod_dim)\n",
        "  for f in range (heatmaps.shape[-1]):\n",
        "    unnormalized_grey_map =  (256 * heatmaps[ :,:, f]).astype(int)\n",
        "    stretched_map = np.expand_dims(np.kron(unnormalized_grey_map, np.ones(kron_prod_dim)), axis=2)\n",
        "    cv2_imshow(stretched_map)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def plot_joints(image, y_pred = [], y_true = []):\n",
        "  assert image is not None\n",
        "\n",
        "  for coordinate in y_pred:\n",
        "    cv2.circle(image, coordinate, 1, (0,0, 255), 2)\n",
        "\n",
        "  for coordinate in y_true:\n",
        "    cv2.circle(image, coordinate, 1, (0,255, 0), 2)\n",
        "\n",
        "  try: \n",
        "    cv2_imshow(image)\n",
        "  except AssertionError as error:\n",
        "    print(repr(error))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xMLLeKSGnx5"
      },
      "source": [
        "Define bounding box class for redimensioning image centered on the subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGghP61lGm91"
      },
      "source": [
        "class BoundingBox():\n",
        "  def __init__(self, points):\n",
        "    self.top_left =  np.min(points, axis=0)\n",
        "    self.bottom_right = np.max(points, axis=0)\n",
        "    self.update_whc()\n",
        "\n",
        "  def update_whc(self):\n",
        "    self.width = np.abs(self.bottom_right[0] - self.top_left[0])\n",
        "    self.height = np.abs(self.bottom_right[1] - self.top_left[1])\n",
        "    self.center = np.array(self.top_left[0] + self.width/2, self.top_left[1] + self.height/2)\n",
        "\n",
        "  def rescale(self, scalingFactor):\n",
        "    # Multiply with unsafe casting\n",
        "    self.top_left =  scalingFactor * self.top_left\n",
        "    self.bottom_right = scalingFactor * self.bottom_right\n",
        "    self.update_whc()\n",
        "\n",
        "  def expand (self, size):\n",
        "    expansion_vector = np.array([(size - self.width)/2, (size - self.height)/2]) \n",
        "    self.top_left -= expansion_vector\n",
        "    self.bottom_right += expansion_vector\n",
        "    self.update_whc()\n",
        "\n",
        "  def tl_br (self):\n",
        "    return tuple(self.top_left.astype(int)), tuple(self.bottom_right.astype(int))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFsnHJMKs7-S"
      },
      "source": [
        "Reorganize matlab structure into pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvDAq6Xvsgyw"
      },
      "source": [
        "MPII_FIELDS = ['NAME','r ankle_X','r ankle_Y', 'r knee_X','r knee_Y', 'r hip_X',\n",
        "               'r hip_Y', 'l hip_X','l hip_Y', 'l knee_X','l knee_Y', 'l ankle_X',\n",
        "               'l ankle_Y','pelvis_X','pelvis_Y','thorax_X','thorax_Y','upper neck_X',\n",
        "               'upper neck_Y', 'head top_X','head top_Y', 'r wrist_X','r wrist_Y',\n",
        "               'r elbow_X','r elbow_Y', 'r shoulder_X','r shoulder_Y','l shoulder_X',\n",
        "               'l shoulder_Y','l elbow_X','l elbow_Y', 'l wrist_X','l wrist_Y','Scale',\n",
        "               'Activity','Category']\n",
        "\n",
        "DATASET_SIZE = 24984 \n",
        "\n",
        "class MpiiDataset():\n",
        "  def __init__(self, image_dir, annotation_path, train=True, csv=False, dataset_size = None, filter = []):\n",
        "    self.network_input_dim = 368\n",
        "    self.train = train\n",
        "    if dataset_size == None:\n",
        "      if self.train:\n",
        "        dataset_size = 17372\n",
        "      else:\n",
        "        dataset_size = 7612\n",
        "    \n",
        "    self.image_dir = image_dir\n",
        "    self.bboxes = {}\n",
        "\n",
        "    print(\"Loading annotations...\")\n",
        "\n",
        "    pb_sample_percent = dataset_size /100\n",
        "\n",
        "    set_size = 0\n",
        "\n",
        "    pb = ProgressBar(total=100, decimals=0, length=50, fill='X', zfill='-')\n",
        "\n",
        "    release = sio.loadmat(annotation_path, struct_as_record=False)['RELEASE']\n",
        "\n",
        "    print(\"Transforming annotations...\")\n",
        "\n",
        "    obj = release[0,0]\n",
        "\n",
        "    annolist = obj.annolist\n",
        "    train_flags = obj.img_train\n",
        "    act = obj.act\n",
        "\n",
        "    self.labels = pd.DataFrame(columns=MPII_FIELDS)\n",
        "\n",
        "    # for each annotated image record\n",
        "    for i in range(0,annolist.shape[1]):\n",
        "\n",
        "      # Only save training or test images\n",
        "      if not train_flags[0,i] == self.train:\n",
        "        continue\n",
        "        \n",
        "      temp = []\n",
        "      obj_list = annolist[0,i]\n",
        "      obj_act = act[i,0]\n",
        "      \n",
        "      rect =obj_list.__dict__['annorect']\n",
        "      img_d = obj_list.__dict__['image']\n",
        "\n",
        "      if rect.shape[0] == 0:\n",
        "        continue\n",
        "          \n",
        "      obj_rect = rect[0,0]\n",
        "      obj_img = img_d[0,0]\n",
        "\n",
        "      if 'annopoints' not in obj_rect._fieldnames:\n",
        "        continue\n",
        "\n",
        "      \n",
        "      # Write image name to record\n",
        "      name = obj_img.__dict__['name'][0]\n",
        "      annopoints = obj_rect.__dict__['annopoints']\n",
        "      \n",
        "      if annopoints.shape[0]==0:\n",
        "        continue\n",
        "        \n",
        "      if not filter == [] and not name in filter:\n",
        "        continue\n",
        "\n",
        "      points = annopoints[0,0].__dict__['point']\n",
        "\n",
        "      temp.append(name)\n",
        "    \n",
        "      # Set default keypoint coordinate value -1\n",
        "      for n in range(0,32):\n",
        "        temp.append(-1)\n",
        "\n",
        "      keypoints = []\n",
        "      # Write keypoints to record\n",
        "      for px in range(0,points.shape[1]):\n",
        "        point = points[0,px]\n",
        "        id = point.__dict__['id']\n",
        "        x = point.__dict__['x']\n",
        "        y = point.__dict__['y']\n",
        "        array_index = 2 * id[0][0] + 1\n",
        "        temp[array_index] = x[0][0]\n",
        "        temp[array_index+1] = y[0][0]\n",
        "        keypoints.append((x[0][0], y[0][0]))\n",
        "      \n",
        "      # Store bboxes in seperate map from dataframe\n",
        "      self.bboxes[str(name)] = BoundingBox(keypoints)\n",
        "\n",
        "      # Write ratio of box size to 200px height\n",
        "      scale = obj_rect.__dict__['scale'][0][0]\n",
        "      temp.append(scale)\n",
        "\n",
        "      # Write activity/category, take the first index if passed list\n",
        "      activity = act[i,0]\n",
        "      activity_name = activity.act_name\n",
        "      category_name = activity.cat_name\n",
        "\n",
        "      if activity_name.shape[0]==0:\n",
        "          temp.append(activity_name)\n",
        "      else:\n",
        "          temp.append(activity_name[0])\n",
        "      if category_name.shape[0]==0:\n",
        "          temp.append(category_name)\n",
        "      else:\n",
        "          temp.append(category_name[0])\n",
        "\n",
        "      self.labels = pd.concat([self.labels, pd.DataFrame([temp],columns=MPII_FIELDS)])\n",
        "\n",
        "      pb.print_progress_bar(int( set_size / pb_sample_percent)) \n",
        "\n",
        "      set_size += 1\n",
        "      if set_size >= dataset_size:\n",
        "        break\n",
        "      \n",
        "    print(\"\\n\" + (\"Training\" if self.train else \"Testing\") + \" annotations dataframe (size \" + str(self.labels.shape) + \") loaded\")\n",
        "\n",
        "    if (csv):\n",
        "      file_name = \"train\" if self.train else \"test\" + '_mpii.csv'\n",
        "      data.to_csv(file_name)\n",
        "      print(\"Dataset written to \" + file_name)\n",
        "\n",
        "  def preprocess_image(self, image_name):\n",
        "\n",
        "    image_path = join(image_dir, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Cache image keypoints and human's bounding box\n",
        "    label = self.labels[self.labels['NAME'].str.contains(image_name)]\n",
        "    bbox = self.bboxes[image_name]\n",
        "\n",
        "    top_left, bottom_right = bbox.tl_br()\n",
        "\n",
        "    # Scale image to have human roughly 200 px in height\n",
        "    targetHeight = 200.0\n",
        "    scalingFactor = targetHeight / bbox.height\n",
        "    image = cv2.resize(image, (0, 0), fx=scalingFactor, fy=scalingFactor)\n",
        "    bbox.rescale((scalingFactor, scalingFactor))\n",
        "\n",
        "    top_left, bottom_right = bbox.tl_br()\n",
        "\n",
        "    bbox.expand(self.network_input_dim)\n",
        "\n",
        "    half_cross = np.full((1, 2), self.network_input_dim / 2).astype(int)[0]\n",
        "    full_cross = np.add(half_cross, half_cross)\n",
        "    half_cross_tuple = (self.network_input_dim // 2,  self.network_input_dim // 2)\n",
        "\n",
        "    # Pad image with black\n",
        "    pad_image = np.pad(image, (half_cross_tuple, half_cross_tuple, (0, 0)), mode='constant')\n",
        "\n",
        "    # Add margin to bounding box top left for cropping start and image diagonal for cropping end\n",
        "    start = np.add(bbox.top_left.astype(int), half_cross)\n",
        "    end =  np.add(start, full_cross)\n",
        "\n",
        "    # Crop image to network input dimensions with human centered\n",
        "    crop_image = pad_image[start[1]:end[1], start[0]:end[0]]\n",
        "\n",
        "    # Perform similar transformations on laabeled annotations\n",
        "    labelX = (np.array(label.iloc[:, 1:32:2])* scalingFactor + half_cross[0] - start[0]).astype(np.int32)[0]\n",
        "    labelY = (np.array(label.iloc[:, 2:33:2])* scalingFactor + half_cross[0] - start[1]).astype(np.int32)[0]\n",
        "    indices = range(0, len(labelX))\n",
        "    \n",
        "    transformed_labels = list(map(lambda x: (labelX[x], labelY[x]), indices))\n",
        "    #transformed_labels = np.hstack([labelY, labelX])\n",
        "\n",
        "    return crop_image, transformed_labels\n",
        "\n",
        "  def get_data(self, validation_split=0.2):\n",
        "    images = [f for f in os.listdir(self.image_dir)if os.path.isfile(os.path.join(self.image_dir, f))]\n",
        "\n",
        "    #label_images = self.labels['NAME'].to_list()\n",
        "    # compute label-image set intersection\n",
        "    dataset = list (set(self.labels['NAME'].to_list()) & set(images))\n",
        "    \n",
        "    print(dataset)\n",
        "\n",
        "    train_len = int(len(dataset) * (1 - validation_split))\n",
        "    #for image_name in dataset[:train_len]:\n",
        "    inputs, y_true = [], []\n",
        "    for image_name in dataset:\n",
        "      #yield self.preprocess_image(image_name)\n",
        "      image, label = self.preprocess_image(image_name)\n",
        "      inputs.append(normalize(image))\n",
        "      y_true.append(label)\n",
        "\n",
        "    return inputs, y_true\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuE-EqIKNxGj"
      },
      "source": [
        "Training function (Work in progress)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RMjlbwhNzBg",
        "outputId": "6a35c51b-5208-47c4-8a9f-dc49088c4be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "image_dir = \"/content/drive/My Drive/Datasets/images/\"\n",
        "annotation_dir = \"/content/drive/My Drive/Datasets/mpii_human_pose_annotation.mat\"\n",
        "\n",
        "num_joints = (len(MPII_FIELDS) - 4) // 2 # Exclude Name, Scale, Activity, Category\n",
        "\n",
        "optimizer = SGD(learning_rate=1e-3)\n",
        "loss_function = MeanSquaredError()\n",
        "\n",
        "cpm = ConvPoseMachine(num_stages = 3, num_parts = num_joints, loss_function = loss_function)\n",
        "\n",
        "def train (model, batch_size = 500, num_epochs = 2):\n",
        "  mpii =  MpiiDataset(image_dir, annotation_dir, dataset_size=batch_size)\n",
        "\n",
        "  x_train, y_train = mpii.get_data()\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "  print (train_dataset)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print(\"\\nBegin epoch %d\" % (epoch,))\n",
        "    epoch_start = time.time()\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_batch_pred = model(x_batch_train, y_batch_train, training=True) \n",
        "\n",
        "\n",
        "train (cpm, 6)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading annotations...\n",
            "Transforming annotations...\n",
            " |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX---------| 83% \n",
            "Training annotations dataframe (size (6, 36)) loaded\n",
            "['005808361.jpg']\n",
            "<BatchDataset shapes: ((None, 368, 368, 3), (None, 16, 2)), types: (tf.float32, tf.int32)>\n",
            "\n",
            "Begin epoch 0\n",
            "conv-shared-1-9x9 - input shape: (1, 368, 368, 3)\n",
            "pool-shared-2 - input shape: (1, 360, 360, 16)\n",
            "conv-shared-2-9x9 - input shape: (1, 180, 180, 16)\n",
            "pool-shared-2 - input shape: (1, 172, 172, 16)\n",
            "conv-shared-3-9x9 - input shape: (1, 86, 86, 16)\n",
            "pool-shared-3 - input shape: (1, 78, 78, 16)\n",
            "conv-shared-4-5x5 - input shape: (1, 39, 39, 16)\n",
            "dense-0-1 - input shape: (1, 35, 35, 16)\n",
            "dense-0-2 - input shape: (1, 35, 35, 16)\n",
            "35\n",
            "Stage 0 loss: 0.000642\n",
            "\n",
            "conv-shared-1-9x9 - input shape: (1, 368, 368, 3)\n",
            "pool-shared-2 - input shape: (1, 360, 360, 16)\n",
            "conv-shared-2-9x9 - input shape: (1, 180, 180, 16)\n",
            "pool-shared-2 - input shape: (1, 172, 172, 16)\n",
            "conv-shared-3-9x9 - input shape: (1, 86, 86, 16)\n",
            "pool-shared-3 - input shape: (1, 78, 78, 16)\n",
            "conv-shared-4-5x5 - input shape: (1, 39, 39, 16)\n",
            "conv-1-11x11 - input shape: (1, 35, 35, 16)\n",
            "conv-1-11x11 - input shape: (1, 25, 25, 16)\n",
            "conv-1-11x11 - input shape: (1, 15, 15, 16)\n",
            "dense-1-1 - input shape: (1, 5, 5, 16)\n",
            "dense-1-2 - input shape: (1, 5, 5, 16)\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-18169cddc353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-18169cddc353>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-1ffa93d4ca33>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, image, y_true)\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0my_true_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_heatmaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-406d8de49ae0>\u001b[0m in \u001b[0;36mbuild_heatmaps\u001b[0;34m(batch_keypoints, heatmap_dim)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimage_keypoints\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_keypoints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mheatmaps\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeypoint_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_keypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mheatmaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-406d8de49ae0>\u001b[0m in \u001b[0;36mkeypoint_gaussian\u001b[0;34m(image, kp, var)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;31m# Copy gaussian to image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mg_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1,5) into shape (1,5,16)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3DeEbL7nYt",
        "outputId": "f83f5ed9-4fa4-4cff-9a55-bad25ef3bdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "image_dir = \"/content/drive/My Drive/Datasets/images/\"\n",
        "annotation_dir = \"/content/drive/My Drive/Datasets/mpii_human_pose_annotation.mat\"\n",
        "\n",
        "def annotate(image, bbox, out_labelX, out_labelY):\n",
        "  anno_image = image.copy()\n",
        "  tl, br = bbox.tl_br()\n",
        "  cv2.rectangle(anno_image, tl, br, (255,0,0), 2, 1)\n",
        "  for i in range (0, len(out_labelX)):\n",
        "    cv2.circle(anno_image, (out_labelX[i], out_labelY[i]), 2, (0,0,255), 2)\n",
        "  print(image.shape)\n",
        "  print(tl, br)\n",
        "  cv2_imshow(anno_image)\n",
        "\n",
        "mpii = MpiiDataset(image_dir, annotation_dir, filter=[\"000001163.jpg\", \"014378517.jpg\"] )\n",
        "\n",
        "if mpii.shuffle:\n",
        "    mpii.labels = mpii.labels.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "image_name = '014378517.jpg'\n",
        "\n",
        "# Load Image\n",
        "image_path = join(image_dir, image_name)\n",
        "print (image_path)\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Cache labels\n",
        "label = mpii.labels[mpii.labels['NAME'].str.contains(image_name)]\n",
        "\n",
        "#Cache bounding box\n",
        "bbox = mpii.bboxes[image_name]\n",
        "\n",
        "top_left, bottom_right = bbox.tl_br()\n",
        "\n",
        "print(top_left, bottom_right) \n",
        "\n",
        "out_labelX = (np.array(label.iloc[:, 1:32:2])).astype(np.int32)[0]\n",
        "out_labelY = (np.array(label.iloc[:, 2:33:2])).astype(np.int32)[0]\n",
        "annotate(image, bbox, out_labelX, out_labelY)\n",
        "\n",
        "print (\"box dim: \" + str((bbox.width, bbox.height)))\n",
        "\n",
        "print (image.shape)\n",
        "\n",
        "# Bounding box target height, scale image to achieve this sizing\n",
        "targetHeight = 200.0\n",
        "scalingFactor = targetHeight / bbox.height\n",
        "\n",
        "print(scalingFactor)\n",
        "\n",
        "image = cv2.resize(image, (0, 0), fx=scalingFactor, fy=scalingFactor)\n",
        "bbox.rescale((scalingFactor, scalingFactor))\n",
        "\n",
        "out_labelX = (np.array(label.iloc[:, 1:32:2])* scalingFactor).astype(np.int32)[0]  \n",
        "out_labelY = (np.array(label.iloc[:, 2:33:2])* scalingFactor).astype(np.int32)[0]\n",
        "annotate(image, bbox, out_labelX, out_labelY)\n",
        "\n",
        "print (\"box dim: \" + str((bbox.width, bbox.height)))\n",
        "\n",
        "top_left, bottom_right = bbox.tl_br()\n",
        "\n",
        "\n",
        "bbox.expand(mpii.network_input_dim)\n",
        "\n",
        "half_cross = np.full((1, 2), mpii.network_input_dim / 2).astype(int)[0]\n",
        "full_cross = np.add(half_cross, half_cross)\n",
        "half_cross_tuple = ( mpii.network_input_dim // 2,  mpii.network_input_dim // 2)\n",
        "\n",
        "print(half_cross)\n",
        "print(full_cross)\n",
        "print(half_cross_tuple)\n",
        "\n",
        "start = np.add(bbox.top_left.astype(int), half_cross)\n",
        "end =  np.add(start, full_cross)\n",
        "\n",
        "print (\"start \" + str(start) + \"\\nend \" + str(end))\n",
        "padded_image = np.pad(image, (half_cross_tuple, half_cross_tuple, (0, 0)), mode='constant')\n",
        "\n",
        "\n",
        "cv2.rectangle(padded_image, tuple(start), tuple(end), (255,255,255), 2)\n",
        "cv2_imshow(padded_image)\n",
        "print(padded_image.shape)\n",
        "\n",
        "cropped_image = padded_image[start[1]:end[1], start[0]:end[0]]\n",
        "cv2_imshow(cropped_image)\n",
        "\n",
        "print(cropped_image.shape)\n",
        "\n",
        "top_left, _ = bbox.tl_br()\n",
        "\n",
        "print(\"top_left \" + str(top_left))\n",
        "print (\"bottom_right \" + str(_))\n",
        "\n",
        "out_labelX = (np.array(label.iloc[:, 1:32:2])* scalingFactor + half_cross[0] - start[0]).astype(np.int32)[0]  \n",
        "out_labelY = (np.array(label.iloc[:, 2:33:2])* scalingFactor + half_cross[0] - start[1]).astype(np.int32)[0]\n",
        "print(out_labelX)\n",
        "print(out_labelY)#\n",
        "out_label = np.hstack([out_labelY, out_labelX])\n",
        "\n",
        "for i in range (0, len(out_labelX)):\n",
        "  cv2.circle(cropped_image, (out_labelX[i], out_labelY[i]), 2, (0,0,255), 2)\n",
        "cv2_imshow(cropped_image)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading annotations...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-ab50f2704285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manno_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmpii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMpiiDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"000001163.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"014378517.jpg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmpii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-e8b8606a7070>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, annotation_path, train, csv, dataset_size, filter)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrelease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct_as_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RELEASE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transforming annotations...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio5.py\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio5.py\u001b[0m in \u001b[0;36mread_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    250\u001b[0m            \u001b[0;31m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         '''\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/copyreg.py\u001b[0m in \u001b[0;36m__newobj__\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__newobj__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__newobj_ex__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}